{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c5d25b",
   "metadata": {},
   "source": [
    "# Evaluation of Tuned Models on Validation Set\n",
    "\n",
    "This notebook loads the best hyperparameters from tuning, trains the models on train set only, and evaluates on validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add paths for imports\n",
    "sys.path.append(\"training\")\n",
    "sys.path.append(\"tuning\")\n",
    "\n",
    "# Import existing functions\n",
    "from split_data import load_splits, rescale_data, print_results\n",
    "from tune_xgb import get_base_xgb_params\n",
    "from tune_lgb import get_base_lgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    \"tuning_results_dir\": \"tuning/results\",\n",
    "    \"output_dir\": \"outputs\",\n",
    "    \"splits_dir\": \"training/splits\",\n",
    "    \"seed\": 42,\n",
    "    \"enable_gpu\": True,\n",
    "    \"models\": [\"xgb\", \"lgb\"],\n",
    "}\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f2c0b",
   "metadata": {},
   "source": [
    "## Load Data and Tuning Results\n",
    "\n",
    "Load the data splits and the best hyperparameters from Optuna studies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits\n",
    "print(\"Loading data splits...\")\n",
    "train_set, val_set, test_set, X_cols, y_cols = load_splits(config[\"splits_dir\"])\n",
    "\n",
    "print(f\"Data loaded successfully:\")\n",
    "print(f\"  Train set: {train_set.shape}\")\n",
    "print(f\"  Validation set: {val_set.shape}\")\n",
    "print(f\"  Test set: {test_set.shape}\")\n",
    "print(f\"  Features: {len(X_cols)}\")\n",
    "print(f\"  Target: {y_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdffef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tuning_results(model_name, tuning_dir):\n",
    "    \"\"\"Load Optuna study results for a model\"\"\"\n",
    "    study_path = f\"{tuning_dir}/{model_name}_optuna_study.pkl\"\n",
    "\n",
    "    if not os.path.exists(study_path):\n",
    "        print(f\"Warning: No tuning results found for {model_name} at {study_path}\")\n",
    "        return None\n",
    "\n",
    "    with open(study_path, \"rb\") as f:\n",
    "        study = pickle.load(f)\n",
    "\n",
    "    print(f\"Loaded tuning results for {model_name.upper()}:\")\n",
    "    print(f\"  Best MAE: {study.best_value:.4f}\")\n",
    "    print(f\"  Number of trials: {len(study.trials)}\")\n",
    "    print(f\"  Best parameters: {study.best_params}\")\n",
    "\n",
    "    return study\n",
    "\n",
    "\n",
    "# Load tuning results for each model\n",
    "tuning_results = {}\n",
    "for model_name in config[\"models\"]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Loading tuning results for {model_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    study = load_tuning_results(model_name, config[\"tuning_results_dir\"])\n",
    "    if study is not None:\n",
    "        tuning_results[model_name] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6419985",
   "metadata": {},
   "source": [
    "## Prepare Model Parameters\n",
    "\n",
    "Combine base parameters with the best hyperparameters from tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9df20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_params(model_name, best_params, seed, enable_gpu):\n",
    "    \"\"\"Get final parameters by combining base params with best tuning params\"\"\"\n",
    "    if model_name == \"xgb\":\n",
    "        base_params = get_base_xgb_params(seed, enable_gpu)\n",
    "    elif model_name == \"lgb\":\n",
    "        base_params = get_base_lgb_params(seed, enable_gpu)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    # Combine base and tuned parameters\n",
    "    final_params = {**base_params, **best_params}\n",
    "    return final_params\n",
    "\n",
    "\n",
    "# Prepare final parameters for each model\n",
    "model_params = {}\n",
    "for model_name, study in tuning_results.items():\n",
    "    print(f\"\\nPreparing parameters for {model_name.upper()}:\")\n",
    "\n",
    "    final_params = get_final_params(\n",
    "        model_name, study.best_params, config[\"seed\"], config[\"enable_gpu\"]\n",
    "    )\n",
    "\n",
    "    model_params[model_name] = final_params\n",
    "\n",
    "    print(f\"  Final parameters: {final_params}\")\n",
    "    print(f\"  Expected validation MAE: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d22c7",
   "metadata": {},
   "source": [
    "## Train Models and Evaluate on Validation Set\n",
    "\n",
    "Train each model on train set only and evaluate on validation set, similar to train_ml.py workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuned_model(model_name, params):\n",
    "    \"\"\"Create model instance with tuned parameters\"\"\"\n",
    "    if model_name == \"xgb\":\n",
    "        return xgb.XGBRegressor(**params)\n",
    "    elif model_name == \"lgb\":\n",
    "        return lgb.LGBMRegressor(**params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "\n",
    "def evaluate_and_save_tuned_results(\n",
    "    model, model_name, val_set, X_cols, y_cols, output_dir\n",
    "):\n",
    "    \"\"\"Evaluate tuned model and save results (similar to train_ml.py logic)\"\"\"\n",
    "    # Make predictions on validation set\n",
    "    val_set_eval = val_set.copy()\n",
    "    val_set_eval[\"y_pred\"] = model.predict(val_set_eval[X_cols])\n",
    "\n",
    "    # Prepare results (same logic as train_ml.py)\n",
    "    val_set_eval = val_set_eval.rename(columns={y_cols[-1]: \"target\"})\n",
    "    val_set_eval = rescale_data(val_set_eval, [\"target\", \"y_pred\"])\n",
    "\n",
    "    # Select output columns\n",
    "    output_columns = [\"Timestamp\", \"Patient_ID\", \"bgClass\", \"target\", \"y_pred\"]\n",
    "    results = val_set_eval[output_columns]\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"\\nEvaluation results for {model_name.upper()} (tuned):\")\n",
    "    print_results(results)\n",
    "\n",
    "    # Save results with \"tuned\" suffix to distinguish from standard models\n",
    "    output_file = f\"{output_dir}/{model_name}_tuned_output.csv\"\n",
    "    results.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_tuned_model(model, model_name, output_dir):\n",
    "    \"\"\"Save the trained tuned model\"\"\"\n",
    "    model_path = f\"{output_dir}/{model_name}_tuned.pickle\"\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"Tuned model saved to: {model_path}\")\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each tuned model\n",
    "results_summary = {}\n",
    "\n",
    "for model_name in model_params.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING AND EVALUATION - {model_name.upper()} (TUNED)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Create model with tuned parameters\n",
    "    print(f\"Creating {model_name.upper()} model with tuned parameters...\")\n",
    "    model = create_tuned_model(model_name, model_params[model_name])\n",
    "\n",
    "    # Train on train set only (not train+val like in final tuning evaluation)\n",
    "    print(f\"Training {model_name.upper()} on train set...\")\n",
    "    print(f\"Train set size: {len(train_set)}\")\n",
    "\n",
    "    model.fit(train_set[X_cols], train_set[y_cols[-1]])\n",
    "    print(f\"Training completed for {model_name.upper()}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    model_path = save_tuned_model(model, model_name, config[\"output_dir\"])\n",
    "\n",
    "    # Evaluate on validation set and save results\n",
    "    print(f\"\\nEvaluating {model_name.upper()} on validation set...\")\n",
    "    print(f\"Validation set size: {len(val_set)}\")\n",
    "\n",
    "    results = evaluate_and_save_tuned_results(\n",
    "        model, model_name, val_set, X_cols, y_cols, config[\"output_dir\"]\n",
    "    )\n",
    "\n",
    "    # Store results for summary\n",
    "    results_summary[model_name] = {\n",
    "        \"model\": model,\n",
    "        \"results\": results,\n",
    "        \"model_path\": model_path,\n",
    "        \"expected_mae\": tuning_results[model_name].best_value,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name.upper()} processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da3171",
   "metadata": {},
   "source": [
    "## Summary and Comparison\n",
    "\n",
    "Compare the results with expected performance from tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate actual MAE for comparison with tuning expectations\n",
    "def calculate_patient_mae(results_df):\n",
    "    \"\"\"Calculate patient-based MAE from results dataframe\"\"\"\n",
    "    patient_maes = []\n",
    "    for patient_id in results_df[\"Patient_ID\"].unique():\n",
    "        patient_data = results_df[results_df[\"Patient_ID\"] == patient_id]\n",
    "        mae = np.mean(np.abs(patient_data[\"target\"] - patient_data[\"y_pred\"]))\n",
    "        patient_maes.append(mae)\n",
    "    return np.mean(patient_maes)\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TUNED MODELS EVALUATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "summary_data = []\n",
    "for model_name, data in results_summary.items():\n",
    "    actual_mae = calculate_patient_mae(data[\"results\"])\n",
    "    expected_mae = data[\"expected_mae\"]\n",
    "    difference = actual_mae - expected_mae\n",
    "\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Model\": f\"{model_name.upper()} (tuned)\",\n",
    "            \"Expected MAE\": f\"{expected_mae:.4f}\",\n",
    "            \"Actual MAE\": f\"{actual_mae:.4f}\",\n",
    "            \"Difference\": f\"{difference:+.4f}\",\n",
    "            \"Output File\": f\"{model_name}_tuned_output.csv\",\n",
    "            \"Model File\": f\"{model_name}_tuned.pickle\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_name.upper()} (TUNED):\")\n",
    "    print(f\"  Expected MAE (from tuning): {expected_mae:.4f}\")\n",
    "    print(f\"  Actual MAE (train->val): {actual_mae:.4f}\")\n",
    "    print(f\"  Difference: {difference:+.4f}\")\n",
    "    print(f\"  Results saved: {config['output_dir']}/{model_name}_tuned_output.csv\")\n",
    "    print(f\"  Model saved: {config['output_dir']}/{model_name}_tuned.pickle\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY TABLE:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{config['output_dir']}/tuned_models_summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea39bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Check output files\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OUTPUT FILES VERIFICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "output_files = (\n",
    "    [f\"{model_name}_tuned_output.csv\" for model_name in config[\"models\"]]\n",
    "    + [f\"{model_name}_tuned.pickle\" for model_name in config[\"models\"]]\n",
    "    + [\"tuned_models_summary.csv\"]\n",
    ")\n",
    "\n",
    "for filename in output_files:\n",
    "    filepath = f\"{config['output_dir']}/{filename}\"\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"✓ {filename} - {size:,} bytes\")\n",
    "    else:\n",
    "        print(f\"✗ {filename} - NOT FOUND\")\n",
    "\n",
    "print(f\"\\nAll tuned models evaluation completed!\")\n",
    "print(f\"Results are saved in: {config['output_dir']}\")\n",
    "print(f\"Files with '_tuned' suffix distinguish them from standard models.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
